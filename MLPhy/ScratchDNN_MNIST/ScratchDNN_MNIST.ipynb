{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DNN from Sctracth with Automatic Differentiation\n",
    "\n",
    "#### Muhammad Gaffar\n",
    "\n",
    "--------------------------------\n",
    "\n",
    "#### A. Using Zygote as Automatic Differentiation\n",
    "\n",
    "Zygote is automatic differentiation package from Julia Programming Language. Automatic differentiation will be used in backward calculation. We can easily compute the gradient of weights and biases from loss function $\\mathcal{L}$ with simple function `gradient`.\n",
    "\n",
    "$$\n",
    "\\partial \\mathbf{W} \\equiv \\frac{\\partial \\mathcal{L}(\\mathbf{W},\\mathbf{b})}{\\partial \\mathbf{W}}\n",
    "$$\n",
    "\n",
    "where $\\mathbf{W}$ and $\\mathbf{b}$ is vector of weights and biases. This is vector gradient.\n",
    "\n",
    "please read \"Mathematics for Machine Learning\" by Marc Peter, et al.\n",
    "\n",
    "First, we load the `Zygote` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Zygote: gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are gonna using the infamous handwritten dataset, MNIST. We just load it from the Flux package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we load MNIST data from Flux Library\n",
    "using Flux.Data.MNIST\n",
    "imgs = MNIST.images();\n",
    "labels = MNIST.labels();\n",
    "\n",
    "#display the MNIST image from given index\n",
    "index = 754\n",
    "display(imgs[index])\n",
    "println(\"label = \", labels[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's see what is the images data is.\n",
    "\n",
    "display(float(imgs[index]));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#And let's see the list of labels\n",
    "\n",
    "display(labels);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pre-processing data to fully-connected (vectors)\n",
    "X = hcat(vec.(float.(imgs))...)\n",
    "\n",
    "\"\"\"\n",
    "convert discretized labels to one_hot vectors \n",
    "    (the vectors that contains bits information, where the index as unique labels)\n",
    "\"\"\"\n",
    "function one_hot(labels)\n",
    "    #number of data\n",
    "    N = length(labels)\n",
    "    \n",
    "    #number of unique labels\n",
    "    num = length(unique(labels))\n",
    "    \n",
    "    #initialize the one-hot vector\n",
    "    y = zeros(Int,num,N)\n",
    "    \n",
    "    #generate one-hot vectors from labels\n",
    "    for i in 1:N\n",
    "        y[:,i] = (labels[i] .== 0:9)\n",
    "    end\n",
    "    return y\n",
    "end\n",
    "\n",
    "y = one_hot(labels)\n",
    "\n",
    "#see the output y \n",
    "display(y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to split the data to train and test data.\n",
    "\n",
    "Train data will be fed to our learning model, and test data will be used as benchmark / accuracy.\n",
    "It is convenient when splitting the data, the initial data must be shuffled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shuffle data and divide to test and train partition\n",
    "using Random\n",
    "\n",
    "\"\"\"\n",
    "shuffle data to test and train data, where the ratio is given by train_perc\n",
    "    \n",
    "    train_prec = ration of train/test\n",
    "\"\"\"\n",
    "function partitionTrainTest(features, labels, train_perc = 0.7)\n",
    "    at = train_perc\n",
    "    N = size(features,2)\n",
    "    idx = shuffle(1:N)\n",
    "    train_idx = view(idx, 1:floor(Int, at*N))\n",
    "    test_idx = view(idx, (floor(Int, at*N)+1):N)\n",
    "    features[:,train_idx,], labels[:,train_idx], features[:,test_idx], labels[:,test_idx]\n",
    "end\n",
    "\n",
    "X_tr, y_tr, X_ts, y_ts = partitionTrainTest(X, y, 2/3);\n",
    "\n",
    "#See the final size of splitting data (train and test)\n",
    "\n",
    "@show size(X_tr,2);\n",
    "@show size(X_ts,2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct struct for model parameters\n",
    "# for simple dense neural network, the model just consist of weights and biases\n",
    "\n",
    "\"\"\"\n",
    "model object, where it contains W and b\n",
    "\"\"\"\n",
    "mutable struct model\n",
    "    W \n",
    "    b\n",
    "end;\n",
    "\n",
    "\"\"\"\n",
    "parameters (weights and biases) initialization of given total layer with normal random\n",
    "\"\"\"\n",
    "function init_params(layer_dims) \n",
    "    #initialize empty model\n",
    "    nn_model = model([],[]) \n",
    "    \n",
    "    #initialize parameters in every layer\n",
    "    for i in 1:length(layer_dims)-1\n",
    "        \n",
    "        #the dimension of weights, W[L,L+1]\n",
    "        W = randn(layer_dims[i],layer_dims[i+1]) .* 0.01\n",
    "        push!(nn_model.W,W)\n",
    "        \n",
    "        #dimension of biases, b[L]\n",
    "        b = zeros(layer_dims[i+1],1)\n",
    "        push!(nn_model.b,b)\n",
    "        \n",
    "    end\n",
    "    return nn_model\n",
    "end\n",
    "\n",
    "#now we construct number of layer, \"0\" for input, \"1,2,3,...\" for hidden layer\n",
    "L0 = size(X,1)\n",
    "L1 = 200\n",
    "L2 = 80\n",
    "L3 = size(y,1)\n",
    "\n",
    "layer = [L0,L1,L2,L3]\n",
    "parameters = init_params(layer);\n",
    "\n",
    "#example, we see weights of layer 2\n",
    "display(parameters.W[2]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the activation function\n",
    "function sigmoid(Z)\n",
    "    A = 1.0 / (1.0 + exp.(-Z))\n",
    "end\n",
    "\n",
    "function relu(Z)\n",
    "    return Z * (Z > 0)\n",
    "end;\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Forward propagation of the model\n",
    "\"\"\"\n",
    "function forward(model,X)\n",
    "    # number of layers\n",
    "    L = Int(length(model.W))\n",
    "    \n",
    "    # forward propagation RELU -> [ RELU ] -> SIGMOID\n",
    "    Z = relu.(model.W[1]' * X .+ model.b[1])\n",
    "    for i in 2:L-1\n",
    "        Z = relu.(model.W[i]' * Z .+ model.b[i])\n",
    "    end\n",
    "    y_pred = sigmoid.(model.W[L]' * Z .+ model.b[L])\n",
    "    \n",
    "    return y_pred\n",
    "end;\n",
    "\n",
    "#example, predicting the output of training data from given parameter models.\n",
    "y_out = forward(parameters,X_tr)\n",
    "\n",
    "display(y_out);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course it still cannot recognize the what is 1, 2, 3 from the data. Then we need to train the data, this is the works of backpropagation or gradient-works.\n",
    "\n",
    "We define the loss function, the function that will be using as the metrics of errorness. The model will learn (taking the gradient of parameters from his mistakes), and using the gradient to optimize the parameters.\n",
    "\n",
    "The loss function we use is L2 norm loss\n",
    "\n",
    "$$\n",
    "\\text{loss} = \\frac{1}{N} \\sum_i (\\hat{y}_i - y_i)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "compute forward and then compute Loss function. L2 norm Loss is used\n",
    "\"\"\"\n",
    "function forward_loss(params,X,y)\n",
    "    N = size(X,2)\n",
    "    y_pred = forward(params,X)\n",
    "    loss   = sum((y_pred - y).^2) ./ N\n",
    "    return loss\n",
    "end;\n",
    "\n",
    "#compute loss for given model, test for 200 data\n",
    "loss = forward_loss(parameters,X_tr[:,200],y_tr[:,200])\n",
    "\n",
    "display(loss);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we test how to compute gradient $\\partial \\mathbf{W}$ with zygote."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test automatic differentiation with zygote for 1000 data\n",
    "dp = gradient(parameters) do m\n",
    "        forward_loss(m,X_tr[:,1000],y_tr[:,1000])\n",
    "    end[1][]\n",
    "\n",
    "#see the gradient of weight in layer 3\n",
    "display(dp.W[3]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now we train the model**\n",
    "\n",
    "1. batch the data\n",
    "2. train every batch. this is called 1 epoch\n",
    "3. train for reasonable epochs\n",
    "\n",
    "**the training procedure is**\n",
    "\n",
    "1. do forward propagation\n",
    "2. compute loss\n",
    "3. compute the gradient\n",
    "4. update parameters\n",
    "\n",
    "    W = W - $\\eta$ dW\n",
    "    \n",
    "    b = b - $\\eta$ db\n",
    "    \n",
    "   where $\\eta$ is the learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we train our model\n",
    "Î· = 0.9\n",
    "L = length(parameters.W)\n",
    "\n",
    "#track our losses over iteration\n",
    "losses = []\n",
    "\n",
    "for epoch in 1:20, i in 1:40\n",
    "\n",
    "        idx = (1 + 1000*(i-1)):1000*i\n",
    "\n",
    "        #forward propagation and compute loss\n",
    "        loss = forward_loss(parameters,X_tr[:,idx],y_tr[:,idx])\n",
    "\n",
    "        #automatic differentiation\n",
    "        dparams = gradient(parameters) do m\n",
    "            return forward_loss(m,X_tr[:,idx],y_tr[:,idx])\n",
    "        end\n",
    "\n",
    "        dparams = dparams[1][]\n",
    "\n",
    "        #update the parameters\n",
    "        parameters.W .-= Î· .* dparams.W\n",
    "        parameters.b .-= Î· .* dparams.b\n",
    "\n",
    "        #print the progress\n",
    "        if i % 20 == 0\n",
    "            append!(losses,loss)\n",
    "            println(\"For epoch \", epoch, \" [ \", 1000*i, \"/40000 ] | Loss = \", loss)\n",
    "        end\n",
    "    \n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we plot the loss\n",
    "\n",
    "using Plots\n",
    "gr()\n",
    "\n",
    "plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's see the accuracy for test data\n",
    "\n",
    "\"\"\"\n",
    "convert back the one-hot vectors, one-cold vectors\n",
    "\"\"\"\n",
    "function one_cold(one_hot_vectors)\n",
    "    N = size(y_ts,2)\n",
    "    \n",
    "    #initialize one_cold_vectors\n",
    "    labels = zeros(Int,N)\n",
    "    \n",
    "    for i in 1:N\n",
    "        #index with maximum probability, -1 comes from the fact the labels are 0:9\n",
    "        predicted = findmax(one_hot_vectors[:,i],dims=1)[2][1] - 1\n",
    "        labels[i] = predicted\n",
    "    end\n",
    "    return labels\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "check accuracy by comparing to the ground truth\n",
    "\"\"\"\n",
    "function accuracy(predicted,truth)\n",
    "    N = length(predicted)\n",
    "    acc = predicted .== truth\n",
    "    return sum(acc) / N    \n",
    "end\n",
    "\n",
    "#predicted labels\n",
    "y_pred_ts = forward(parameters,X_ts)\n",
    "\n",
    "#convert to one_cold\n",
    "predicted = one_cold(y_pred_ts)\n",
    "truth = one_cold(y_ts)\n",
    "\n",
    "#the accuracy\n",
    "acc = accuracy(predicted,truth)\n",
    "\n",
    "println(\"Accuracy for test data = \", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicted labels\n",
    "y_pred_tr = forward(parameters,X_tr)\n",
    "\n",
    "#convert to one_cold\n",
    "predicted = one_cold(y_pred_tr)\n",
    "truth = one_cold(y_tr)\n",
    "\n",
    "#the accuracy\n",
    "acc = accuracy(predicted,truth)\n",
    "\n",
    "println(\"Accuracy for train data = \", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that just fromm simple 3 dense layer, without overparameterized model can do so well."
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.3.0",
   "language": "julia",
   "name": "julia-1.3"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.3.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
