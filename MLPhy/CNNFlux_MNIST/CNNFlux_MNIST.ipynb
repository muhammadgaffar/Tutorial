{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network with Flux Framework\n",
    "\n",
    "#### Muhammad Gaffar\n",
    "----------------------------------\n",
    "\n",
    "It is known that to get more generalized model for complex data, we need more bigger parameters and layers on our deep neural network. But, to do that, we also need bigger computational resources. To tackle this problem, Yann LeCunn proposed in 1988 a convolution network. A network that can compress the data, and also figuring out the correlation in subspace of the data by gradient descent. \n",
    "\n",
    "Here's a simple architecture of CNN\n",
    "\n",
    "![arch](fig/CNN_arch.png)\n",
    "\n",
    "*image is retrieved from introduction to cnn from class cs231a stanford*\n",
    "\n",
    "what is CNN do is that there are filter layer that have less size than the data, that the filter with weight inside it will slide across the data and calculte the dot product of it. The output data is can be seen as compressed data.\n",
    "\n",
    "![filter](fig/ConvFilter.png)\n",
    "\n",
    "\n",
    "There are also a padding and stride, but that is not central on CNN. They are just additional tools to get the compressed data size right. The final volume size of the data is\n",
    "\n",
    "$$\n",
    "W_2 = \\frac{W_1 - F + 2P}{S} + 1\n",
    "$$\n",
    "\n",
    "where $W_1$ is the previous or input weight, $F$ is size of the filter, $P$ is size of zero padding. For height same as weight. and for number of channels\n",
    "\n",
    "$$\n",
    "D = K\n",
    "$$\n",
    "\n",
    "where $K$ is number of filter\n",
    "\n",
    "In this tutorial, I will shown the CNN easily using Flux framework.\n",
    "\n",
    "We load the FLUX and the our toy data (MNIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Flux\n",
    "using Flux.Data.MNIST\n",
    "\n",
    "# Now we load MNIST data from Flux Library\n",
    "imgs = MNIST.images(:train);\n",
    "labels = MNIST.labels(:train);\n",
    "\n",
    "#display the MNIST image from given index\n",
    "index = 5491\n",
    "display(imgs[index])\n",
    "println(\"label = \", labels[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we are gonna need some tools.\n",
    "using Flux: onehotbatch, onecold\n",
    "using Base.Iterators: partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "make minibatch data \n",
    "    X = images data\n",
    "    Y = labels data\n",
    "    idxs = vector range of index\n",
    "\"\"\"\n",
    "function make_minibatch(X, Y, idxs)\n",
    "    #initialize array, with dimension (Height,Width,Channel,BatchIndex) WHCN order\n",
    "    #this is used, so conv filter in flux can be used\n",
    "    X_batch = Array{Float32}(undef, size(X[1])..., 1, length(idxs))\n",
    "    #fill array data to minibatch\n",
    "    for i in 1:length(idxs)\n",
    "        X_batch[:, :, :, i] = Float32.(X[idxs[i]])\n",
    "    end\n",
    "    Y_batch = onehotbatch(Y[idxs], 0:9)\n",
    "    return (X_batch, Y_batch)\n",
    "end\n",
    "\n",
    "# let's see first what onehotbatch data do\n",
    "onehotbatch(labels[1:10],0:9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we are gonna use partition function that we get from base.iterators, let's see what partition do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we use partition function as list of index that contain every batch\n",
    "#minibatch index\n",
    "batch_size = 256\n",
    "mb_idxs = partition(1:length(imgs), batch_size);\n",
    "\n",
    "#let's see the minibatch indexes\n",
    "collect(mb_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we make the dataset by from minibatch index\n",
    "train_set = [make_minibatch(imgs, labels, i) for i in mb_idxs];\n",
    "\n",
    "#load test data and convert to one batch only\n",
    "test_imgs = MNIST.images(:test)\n",
    "test_labels = MNIST.labels(:test)\n",
    "test_set = make_minibatch(test_imgs, test_labels, 1:length(test_imgs));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flux gave us a simple way to build model, using chain function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we build our very simple model, consist of three conv layer, and one dense layer\n",
    "model = Chain(\n",
    "    # First convolution, operating upon a 28x28 image\n",
    "    Conv((3, 3), 1=>16, pad=(1,1), relu),\n",
    "    MaxPool((2,2)),\n",
    "\n",
    "    # Second convolution, operating upon a 14x14 image\n",
    "    Conv((3, 3), 16=>32, pad=(1,1), relu),\n",
    "    MaxPool((2,2)),\n",
    "    \n",
    "    # Third convolution, operating upon a 7x7 image\n",
    "    Conv((3, 3), 32=>32, pad=(1,1), relu),\n",
    "    MaxPool((2,2)),\n",
    "\n",
    "    # Flatten the data , 3x3x32 = 288\n",
    "    # which is where we get the 288 in the `Dense` layer below:\n",
    "    x -> reshape(x, :, size(x, 4)),\n",
    "    Dense(288, 10),\n",
    "\n",
    "    # Finally, softmax to get nice probabilities\n",
    "    softmax,\n",
    ")\n",
    "\n",
    "#compile the model\n",
    "model(train_set[1][1]);\n",
    "\n",
    "#now we define our loss function using crossentropy\n",
    "using Flux: crossentropy\n",
    "loss(x,y) = crossentropy(model(x),y)\n",
    "\n",
    "#model accuracy\n",
    "using Statistics\n",
    "accuracy(x,y) = mean(onecold(model(x)) .== onecold(y));\n",
    "\n",
    "#and the optimizer using gradient descent\n",
    "η = 0.005 #learning rate\n",
    "opt = Descent(η);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we train our model using function within flux, `Flux.train!`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@time Flux.train!(loss,params(model),train_set,opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in 1:15\n",
    "    # Train for a single epoch\n",
    "    Flux.train!(loss, params(model), train_set, opt)\n",
    "\n",
    "    # Calculate accuracy:\n",
    "    acc = accuracy(test_set...)\n",
    "    println(\"Epoch $epoch ==> Accuracy = $acc\")\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "even though for full batch train require much more time, we see that for minimum, give epoch high accuracy result. This indeed will be useful if we train in more complex data, thus ConvNets give more efficiency."
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.3.0",
   "language": "julia",
   "name": "julia-1.3"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.3.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
